---
title: "Autocorrelation"
author: "Minah Ramandrosoa"
date: "2024-06"
output: github_document
---

```{r setup, include=FALSE}
# This chunk is usually used to set global options and load libraries
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


In linear regression, correlated error terms can introduce bias and inconsistency into the Ordinary Least Squares estimates of the regression coefficients, thereby compromising predictive performance of the model. 

Autocorrelation occurs when the errors terms $\varepsilon_i$ and $\varepsilon_j$ are correlated, expressed by $Cov(\varepsilon_i, \varepsilon_j)\neq 0$


Given the simple linear regression model with first-order autoregressive errors : 

$y_i = \beta_0 + \beta_1x_i + \varepsilon_i$ where the error terms $\varepsilon_i$ are correlated.
  
  - $y_i$ and $x_i$ are the observed values of the response and predictor variables at time $i$

  - Correlated error terms often follow a first order autoregressive process AR(1) and can be expressed as follow:

$$
\varepsilon_i = \phi \varepsilon_{i-1} + a_i , a_i \sim N(0,\sigma^2)
$$
 
  - $\phi$ is an unknown parameter that defines the relationship between successive model errors $\varepsilon_i$ and $\varepsilon_{i-1}$.  

  - $a_i$ is a random variable normally and independently distributed. 


To detect the presence of autocorrelation in a linear regression model, we can deploy a Durbin-Watson (DW) test. It tests for first order serial correlation, which means it checks whether the residuals are correlated with each other across time.

The correlation coefficient between of $\varepsilon_i$ and $\varepsilon_{i-1}$ is given by:
  
$$
Corr(\varepsilon_i, \varepsilon_{i-1}) = \frac{Cov(\varepsilon_{i-1}, \varepsilon_i)} {\sqrt{Var(\varepsilon_i)}\sqrt{Var(\varepsilon_{i-1}})}
$$

Assume the stationary process where the mean and variance of the error term are constant over time, then 

$$
Var(\varepsilon_{i-1}) = Var(\varepsilon_i) = Var(\varepsilon_{i+1}) = ... 
$$

Recall $\varepsilon_i = \phi \varepsilon_{i-1} + a_i$  where  $a_i\sim N(0,\sigma^2)$

We can rewrite $Var(\varepsilon_i) = Var(\phi \varepsilon_{i-1} + a_i)$ 

The variances $Var(\phi \varepsilon_{i-1})$ and $Var(a_i)$ are independent so they are additive.By applying the scaling property of the variance when multiplied by a constant ($\phi$ is the constant), we get:

  $Var(\varepsilon_i) = \phi^2Var(\varepsilon_{i-1}) + Var(a_i)$
    
  $Var(\varepsilon_i) = \phi^2Var(\varepsilon_i) + \sigma^2$
  
$$
Var(\varepsilon_i) = \frac{\sigma^2} {(1-\phi^2)}
$$
    
  
While $Cov(\varepsilon_{i-1}, \varepsilon_i) = Cov(\varepsilon_{i-1}, \phi \varepsilon_{i-1}+a_i,)$

Applying the linearity of covariance : 

$Cov(\varepsilon_{i-1}, \varepsilon_i) = Cov(\phi \varepsilon_{i-1}, \varepsilon_{i-1})+ Cov(a_i, \varepsilon_{i-1})$

$Cov(\varepsilon_{i-1}, \varepsilon_i) = \phi Cov(\varepsilon_{i-1}, \varepsilon_{i-1})+ Cov(a_i, \varepsilon_{i-1})$

As $a_i$ are noise , the $Cov(a_i, \varepsilon_{i-1})$ =0

$Cov(\varepsilon_{i-1}, \varepsilon_i) = \phi Cov(\varepsilon_{i-1}, \varepsilon_{i-1})$

$Cov(\varepsilon_{i-1}, \varepsilon_i) = \phi Var(\varepsilon_{i-1})$

Recall $Var(\varepsilon_{i-1}) = Var(\varepsilon_i) = \frac {\sigma^2}{(1-\phi^2)}$

Then 

$$
Cov(\varepsilon_{i-1}, \varepsilon_i) = \phi \frac{\sigma^2} {(1-\phi^2)}
$$
  
Recall, 

  
$$
Corr(\varepsilon_i, E{i-1}) = \frac{Cov(\varepsilon_{i-1}, \varepsilon_i)} {\sqrt{Var(\varepsilon_i)}\sqrt{Var(\varepsilon_{i-1}})}
$$


$$
\text{Corr}(\varepsilon_i, \varepsilon_{i-1}) = \frac{\phi \frac{\sigma^2}{1-\phi^2}}{\sqrt{\frac{\sigma^2}{1-\phi^2}} \sqrt{\frac{\sigma^2}{1-\phi^2}}}
$$


$$
Corr(\varepsilon_i, \varepsilon_{i-1}) = \phi
$$


Therefore, the parameter $\phi$ is also the correlation coefficient. 

The lag k autocorrelation is given by $p_k = \phi^k$ for $k =1,2,...,$ this equation is called the autocorrelation function. 

The lag one autocorrelation is given by : 


$$
p_1 = Corr(\varepsilon_i, \varepsilon_{i-1}) 
$$

$$
p_1= \phi
$$
 

The Durbin-Watson Test only checks for autocorrelation with a lag of 1. 

Given the Durbin-Watson test statistic :


$$
d = \frac{\Sigma_{i=1}^{n}(\varepsilon_{i+1}-\varepsilon_{i})^2}{\Sigma_{i=1}^{n} \varepsilon_i^2}
$$

Let us express $d$ as a function of the estimation $\hat{\phi}$. 

Recall:

$\varepsilon_{i+1}$ = $\phi \varepsilon_{i} + a_i$  ,  $a_i \sim N(0,\sigma^2)$ 

This relationship resembles a typical linear regression form:

$y_i$ = $\beta_1 x_{i} + \varepsilon_i$ , $\varepsilon_i$ ~$N(0,\sigma^2)$ 

We can estimate $\phi$ by using the Ordinary Least Squares regression to approximate the slope :


$$
\hat{\phi}  = \frac{\Sigma_{i=1}^{n}(\varepsilon_i - \bar{\varepsilon})(\varepsilon_{i+1} - \bar{\varepsilon})}{\Sigma_{i=1}^{n}(\varepsilon_i - \bar{\varepsilon})^2}
$$

**Notice that this estimation is the correlation coefficient between $\varepsilon_i$ and $\varepsilon_{i+1}$ when the mean and variance of the error terms are constant over time (stationary process)**

We know that the expected value of the error terms $\varepsilon_i$ is 0 then : 


$$
\hat{\phi} = \frac{\Sigma_{i=1}^{n} \varepsilon_i\varepsilon_{i+1}}{\Sigma_{i=1}^{n} \varepsilon_i^2}
$$

The developed form of the Durbin-Watson statistic is as follow :


$$
d = \frac{\Sigma_{i=1}^{n} (\varepsilon_{i}^2-2\varepsilon_i\varepsilon_{i+1}+\varepsilon_{i+1}^2)}{\Sigma_{i=1}^{n} \varepsilon_i^2}
$$


$$
d = \frac{\Sigma_{i=1}^{n} (\varepsilon_{i+1}^2)}{\Sigma_{i=1}^{n}  \varepsilon_i^2} + \frac{\Sigma_{i=1}^{n} (\varepsilon_{i}^2)}{\Sigma_{i=1}^{n}  \varepsilon_i^2} -\frac{\Sigma_{i=1}^{n} (2\varepsilon_i\varepsilon_{i+1})}{\Sigma_{i=1}^{n}  \varepsilon_i^2}
$$


For large n ,  ${\Sigma_{i=1}^{n} (\varepsilon_{i+1}^2)}\approx\Sigma_{i=1}^{n}  \varepsilon_i^2$ 


Then , we get : 


$$
d\approx 2  -\frac{\Sigma_{i=1}^{n}(2\varepsilon_i\varepsilon_{i+1})}{\Sigma_{i=1}^{n} \varepsilon_i^2} 
$$

$$
d\approx 2(1-\frac{\Sigma_{i=1}^{n} \varepsilon_i\varepsilon_{i+1}}{\Sigma_{i=1}^{n} \varepsilon_i^2})
$$


$$
d\approx 2(1-\hat{\phi})
$$

When $d$ tends to 2, we will have no autocorrelation as $\hat{\phi} \approx 0$

When $d$ tends to 0, hence $\hat{\phi} \approx 1$, we will have a perfect autocorrelation. 

Instead of using a precise critical value for $d$, the decision procedure is as follows:

  - if $d < d_L$, reject $H_0 : \phi =0$
  - if $d > d_U$, do not reject $H_0 : \phi =0$
  - if $d_L \leq d \leq d_U$, the test is inconclusive
  
We need to refer to statistical tables to determine the critical values $d_U$ and $d_L$  because they depend on the sample size (n) and the number of regressors (k) in the model. 

To detect negative correlation, we consider values of $d$ greater than 2, as negative correlation corresponds to $d$ values above 2. 

  - if $d > 4-d_L$, do not reject $H_0 : \phi =0$
  - if $d < 4-d_U$, reject $H_0 : \phi =0$
  - if $4-d_U \leq d \leq 4-d_L$, the test is inconclusive. 
  
Additionally, we can use the p-value to make an inference. 

  - p-value < $\alpha$, reject $H_0 : \phi =0$
  - p-value > $\alpha$, do not reject $H_0 : \phi =0$

```{r}
data <- read.csv("C:/Users/USER/Desktop/IIT/data_autocorr.csv")
```

```{r}
library(lmtest)
```


```{r}
  #Perform Durbin-Watson test 
x <- data$x
y <- data$y

model <- lm(y~x)
dwtest(model)
```


We see that the test statistics is d = 0.4116 which is close to 0,hence $\phi$ approaches 1, and the p-value is less than the significance level 0.05. Then we reject the null hypothesis $H_0 : \phi$ = 0 and we conclude that the errors $\varepsilon_i$ are correlated. 

To further investigate, let us examine the ACF plot 

```{r}
library(tseries)
acf(resid(model))
```

This plot shows the correlation of the error terms in different lags. The blue dotted line represents the significance level. 

Lag 0 represents the correlation of a residual with itself, then it is always taken as one. 

If there is autocorrelation, the vertical bars would quickly drop to almost zero or at least between or ear the blue line. 

For instance, we can conclude that the residuals are correlated but the correlation ceases after lag 5.

To address to this autocorrelation issue, we will to perform an appropriate transformation of variables. 

Recall $y_i$ and $x_i$ are the observations  of the response and predictor variables at time $i$, let us define new variables $y_i'$ :

$$
y_i' = y_i - \phi y_{i-1}
$$

and $x_i'$:


$$
x_i' = x_i - \phi x_{i-1}
$$

We will demonstrate how these transformations lead to a new model with uncorrelated residuals.

Starting with the initial linear regression model :

$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ and $\varepsilon_i = \phi \varepsilon_{i-1} + a_i$ , where $a_i$ ~NID(0, $\sigma^2$ )

By substituting :

$y_i' = y_i - \phi y_{i-1}$


$y_i' = (\beta_0 + \beta_1x_i +\varepsilon_i) - \phi(\beta_0 + \beta_1x_{i-1} +\varepsilon_{i-1})$


$y_i' = \beta_0 (1-\phi) + \beta_1(x_i -\phi x_{i-1}) + \varepsilon_i-\phi E{i-1}$


Thus, 


$$
y_i' = \beta_0 (1-\phi) + \beta_1x_i' + a_i
$$


Here, $a_i$ represents residuals that are uncorrelated and exhibit constant variance (white noise). 

Additionally, $\beta_0' = \beta_0 (1-\phi)$ represents the transformed y intercept, enhancing the model's performance. 

$\phi$ will be approximated by using $p_1 = corr(\varepsilon_i, E{i-1})$

```{r}
par(mfrow=c(1,2))
  #extract and compute the ACF of the residuals
rho = acf(resid(model))

  #Extract the ACF value at lag 1
r = rho[1]$acf[1,,]

n = length(model$residuals)

  #Transform the dependent and the independent variables 
y_prime = rep(0,n-1)
x_prime = rep(0,n-1)

for (i in 1:n-1){
  y_prime[i] = data$y[i+1]-r*data$y[i]
  x_prime[i] = data$x[i+1]-r*data$x[i]
}

model2<- lm(y_prime~x_prime)

acf(resid(model2))

```

We set up Model 2 using the transformed dependent and independent variables. From the ACF plot, we observe that the correlation drops directly to 0 at lag 1 when using the transformed variables. This suggests that the residuals are no more correlated to each other. 

Let us perform a Durbin-Watson test to confirm that. 

```{r}
dwtest(model2)
```

The test statistic d is now closer to 2, which suggests a correlation coefficient $\phi \approx 0$. In addition, the p-value is greater than the significance level of 0.05,then we fail to reject the null hypothesis $H_0 : \phi$ = 0 and we conclude that the errors $\varepsilon_i$ are not correlated. 
